"Section","Subsection","Front","Back","Tag"
"01 First Year Statistics","a Set Theory","Sample Space","The set [$]S[/$] of all possible outcomes of a particular experiment",""
"01 First Year Statistics","a Set Theory","Event (Set Theory)","An event is any collection of possible outcomes of an experiment.  That is, any subset of [$]S[/$] including itself.",""
"01 First Year Statistics","a Set Theory","Subset","Set A is a subset of B if all elements in A are also in B  That is:   [$]A \subset B[/$]   [$]\Longleftrightarrow [/$]  [$]x \in A \Longrightarrow x \in B[/$]",""
"01 First Year Statistics","a Set Theory","Union (Set Theory)","The union of A and B is the set of elements that belong to either A or B or both  [$]A \cup B = \{x: x \in A \text{ or } x \in B\}[/$]",""
"01 First Year Statistics","a Set Theory","Intersection (Set Theory)","The intersection of A and B is the set of elements that belong to both A and B.  [$]A \cap B = \{x : x  \in A \text{ and } x \in B\}[/$]",""
"01 First Year Statistics","a Set Theory","Complement (Set Theory)","The complement of A is the set of all elements that are not in A:  [$]A^c = \{x : x \notin A\}[/$]",""
"01 First Year Statistics","a Set Theory","DeMorgan's Laws","Complement of a union or intersection of two sets:  [$](A \cap B)^c = A^c \cup B^c[/$]  [$](A \cup B)^c = A^c \cap B^c[/$]",""
"01 First Year Statistics","a Set Theory","Disjoint (Mutually Exclusive)","Two sets A and B are disjoint (mutually exclusive) if  [$]A \cap B = \emptyset[/$]  They have no elements in common.",""
"01 First Year Statistics","a Set Theory","Partition (Set Theory)","A partition is a collection of sets such that they are pairwise disjoint.  That is, no two sets share anything in common.",""
"01 First Year Statistics","b Probability","Sigma Algebra","A collection of subsets of set S is called a sigma algebra, denoted by [$]\mathcal{B}[/$] if it satisfies these three properties:  1) [$]\emptyset \in \mathcal{B}[/$]  2) If [$]A \in \mathcal{B}[/$], then [$]A^c \in \mathcal{B}[/$]  3) If [$]A_1, A_2, \ldots A_n \in \mathcal{B}[/$], then [$]\bigcup_{i=1}^n A_i \in \mathcal{B}[/$]  Property 2 says sigma algebras are ""closed under complementation""  Property 3 says sigma algebras are ""closed under countable unions""",""
"01 First Year Statistics","b Probability","Probability Function","With a sigma algebra [$]\mathcal{B}[/$], a probability function is a function [$]P[/$] with domain [$]\mathcal{B}[/$] that satisfies:  1) [$]P(A) \geq 0[/$] for all [$]A \in \mathcal{B}[/$]  2) [$]P(S) = 1[/$]  3) If [$]A_1, A_2, \ldots, \in \mathcal{B}[/$] are pairwise disjoint, then:  [$]P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)[/$]  These three properties are also known as ""the axioms of probability""",""
"01 First Year Statistics","b Probability","What is the probability of the empty set?  [$]P(\emptyset) = ?[/$]","[$]P(\emptyset) = 0[/$]",""
"01 First Year Statistics","b Probability","What is the probability of an event bounded by?  [$]P(A) \leq ?[/$]","[$]P(A) \leq 1[/$]",""
"01 First Year Statistics","b Probability","How do the probabilities of A and A complement relate to each other?  [$]P(A) \questeq P(A^c)[/$]","[$]P(A^c) = 1 - P(A)[/$]  or   [$]P(A) + P(A^c) = 1[/$]",""
"01 First Year Statistics","b Probability","Bonferroni's Inequality","[$]P(A \cap B) \geq P(A) + P(B) - 1[/$]",""
"01 First Year Statistics","b Probability","Boole's Inequality","For any sets [$]A_1, A_2, \ldots,[/$]   [$]P\left(\bigcup_{i=1}^{\infty}A_i\right)\leq \sum_{i=1}^{\infty} P(A_i)[/$]   The union may have overlapping area that is overcounted",""
"01 First Year Statistics","c Counting","Permutation Formula  [$]_nP_r = ?[/$]","[$]_nP_r = \frac{n!}{(n-r)!}[/$]","Intro::Stats"
"01 First Year Statistics","c Counting","Combination Formula  [$] \binom{n}{r} = ?[/$]","[$] \binom{n}{r} = \frac{n!}{(n-r)! r!}[/$]","1_Intro_Stats"
"01 First Year Statistics","c Counting","Fundamental Theorem of Counting","If a job consists of k separate tasks, the i-th of which can be done in n_i ways, then the entire job can be done in  [$]n_1 \times n_2 \times \cdots \times n_k[/$] ways",""
"01 First Year Statistics","c Counting","Factorial","[$]n! = n \times (n-1) \times \cdots \times 2 \times 1[/$]",""
"01 First Year Statistics","c Counting","How many ways to select k out of n objects:  1) Ordered, without replacement 2) Ordered, with replacement 3) Unordered, without replacement 4) Unordered, with replacement","[$]1) \frac{n!}{(n-k)!}[/$]  [$]2) n^k[/$]  [$]3) \frac{n!}{(n-k)! k!} = \binom{n}{k}[/$]  [$]4) \binom{n+k-1}{k}[/$]",""
"01 First Year Statistics","c Counting","What does [$]\binom{n}{r}[/$] stand for","It means ""n choose r"". Out of n objects, how many ways are there to select r objects when ordering doesn't matter.  [$]\binom{n}{r} = \frac{n!}{(n-r)!r!}[/$]",""
"01 First Year Statistics","d Conditional Probability and Independence","Conditional Probability  [$]P(A | B) = ?[/$]","[$]P(A | B) = \frac{P(A \cap B)}{P(B)}[/$]",""
"01 First Year Statistics","d Conditional Probability and Independence","Bayes Rule","Univariate case:  [$]P(A | B) = \frac{P(B | A)P(A)}{P(B)}[/$]  Multivariate case:  [$]P(A_i | B) = \frac{P(B|A_i) P(A_i)}{\sum_{i=1}^{\infty} P(B | A_j)P(A_j)}[/$]",""
"01 First Year Statistics","d Conditional Probability and Independence","What does it mean for two events to be independent?","[$]P(A \cap B) = P(A) P(B)[/$]  The probability of seeing both events is just the product of their individual probabilities.",""
"01 First Year Statistics","d Conditional Probability and Independence","Pairwise vs Mutual Independence","Pairwise:  [$]P(A_i \cap A_j) = P(A_i) P(A_j)[/$] for any [$]i \neq j[/$]  Mutual:  A collection of events [$]A_1, \ldots, A_n[/$] are mutually independent if for any SUBCOLLECTION, [$]A_{i_1}, A_{i_2}, \ldots A_{i_k}[/$], we get  [$] P\left(\bigcap_{j=1}^k A_{i_j}\right) = \prod_{j=1}^k P(A_{i_j})[/$]  Mutual independence is stronger than pairwise independent. Mutual independence implies pairwise independence, but not the other way around",""
"01 First Year Statistics","e Random Variables","Cumulative Distribution Function (CDF)","The probability that [$]X[/$] is less than or equal to a specified value [$]x[/$]:  [$]F_X(x) = P_X(X \leq x)[/$]","1_Intro_Stats"
"01 First Year Statistics","e Random Variables","CDF formula of a continuous/discrete random variable with respect to its PDF/PMF","Continuous:  [$]F_X(x) = \int_{-\infty}^x f_X(t) dt[/$]  Discrete:  [$]F_X(x) = \sum_i P(X = i)[/$]   for [$]i \leq x[/$]","1_Intro_Stats"
"01 First Year Statistics","e Random Variables","CDF for discrete random variable","[$]F_X(x) = \sum_i P(X = i)[/$]   for [$]i \leq x[/$]","1_Intro_Stats"
"01 First Year Statistics","e Random Variables","Probability Mass Function (PMF) vs Probability Distribution Function (PDF)","PMF is only relevant to discrete RVs, while a PDF is only relevant to continuous RVs.   The PMF of X is:  [$]f_X(x) = P(X = x)[/$]   for all [$]x[/$]  The PDF of X is a function that is commonly written as [$]f_X(x)[/$] and is defined to satisfy:  [$]F_X(x) = \int_{-\infty}^x f_X(t) dt[/$]","1_Intro_Stats"
"01 First Year Statistics","e Random Variables","How do the CDF and PDF relate to each other?","The derivative of the CDF is the PDF  [$]f_X(x) = \frac{d}{dx} F_X(x)[/$]  The integral of the PDF is the CDF  [$]\int_{-\infty}^x f_X(u) du = F_X(x)[/$]",""
"01 First Year Statistics","e Random Variables","Random Variable","A random variable is a function from a sample space S into the real numbers.",""
"01 First Year Statistics","e Random Variables","Support (Random Variables)","The support of a random variable is the set of values it can take on.  It is also commonly called the ""range"".  Ex: support of unif[0, 1] is [0, 1]",""
"01 First Year Statistics","e Random Variables","Properties of CDFs  1) Limits to infinity and negative infinity 2) How does it increase/decrease? 3) When is it continuous?","[$]1) \lim_{n \rightarrow -\infty}F(x) = 0, \lim_{n \rightarrow \infty} F(x) = 1[/$]  2) F(x) is a nondecreasing function of x  3) F(x) is right-continuous; that is, for every number x_0:  [$]\lim_{x \downarrow x_0}F(x) = F(x_0)[/$]",""
"01 First Year Statistics","e Random Variables","Discrete vs continuous random variable","Definition 1:  A random variable X is continuous if its CDF is a continuous function.   A random variable X is discrete if its CDF is a step function.  Definition 2:   A continuous random variable can take on a countably infinite amount of realizations.   A discrete random variable can take on a countably finite amount of realizations.",""
"01 First Year Statistics","e Random Variables","Identically Distributed","Random variables X and Y are identically distributed if for every set [$]A \in \mathcal{B}[/$],   [$]P(X \in A) = P(Y \in A)[/$]  HOWEVER, being identically distributed does not necesarily mean they are equal.   For example, lets flip a fair coin 5 times. The # of heads is identically distributed to the # of tails. But when realized, they are NEVER equal.",""
"01 First Year Statistics","e Random Variables","What does [$]X \sim Y[/$] mean?","Random variables X and Y are identically distributed.  The [$]\sim[/$] symbol means ""is distributed as""  Ex: [$]X \sim \mathcal{N}(0, 1)[/$]",""
"01 First Year Statistics","e Random Variables","Properties of a PMF/PDF  1) What do its values take on? 2) What do its values sum up to?","Let [$]f_X(x)[/$] be the PDF/PMF. Then:  [$]1) f_X(x) \geq 0[/$] for all x  That is, the PDF/PMF can only take on non-negative values  [$]2) \sum_x f_X(x) = 1, \int_{-\infty}^{\infty} f_X(x) dx = 1[/$]  That is, the PDF/PMF sums/integrates to 1.",""
"01 First Year Statistics","h Distributions","Density of Normal Random Variable","[$]f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{1}{2\sigma^2}(x - \mu)^2\right)}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Expectation of Normal Random Variable","[$]\mathbb{E}[X] = \mu[/$]",""
"01 First Year Statistics","h Distributions","Bernoulli Distribution PDF","[$]f_X(x | p) = \begin{cases} P(X = 1) = p\\ P(X = 0) = 1 - p\\ \end{cases}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Bernoulli Distribution Mean","[$]\mathbb{E}[X] = p[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Bernoulli Distribution Variance","[$]Var(X) = p(1-p)[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Bernoulli Distribution MGF","[$]\mathbb{E}[e^{tX}] = 1 - p + p\cdot e^{t}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Binomial Distribution PMF","[$]P(X = k | n, p) = \binom{n}{k} p^k (1 - p)^{n-k}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Binomial Distribution Mean","[$]\mathbb{E}[X] = np [/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Binomial Distribution Variance","[$]Var(X) = np(1-p)[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Binomial Distribution MGF","[$]\mathbb{E}[e^{tX}] = (1 - p + p \cdot e^{t})^n[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Geometric Distribution PMF","[$]P(X = k | p) = (1 - p)^{k-1} p[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Geometric Distribution Mean","[$]\mathbb{E}[X] = \frac{1}{p}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Geometric Distribution Variance","[$]Var(X) = \frac{1-p}{p^2}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Geometric Distribution MGF","[$]\mathbb{E}[e^{tX}] = \frac{p e^t}{1 - (1 - p)e^t}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Negative Binomial Distribution PMF","[$]P(X = k | p, r) = \binom{k+r-1}{k}(1 - p)^r p^k[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Negative Binomial Distribution Mean","[$]\mathbb{E}[X] = \frac{pr}{1-p}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Negative Binomial Distribution Variance","[$]Var(X) = \frac{pr}{(1-p)^2}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Negative Binomial Distribution MGF","[$]\mathbb{E}[e^{tX}] = \left(\frac{1 - p}{1 - pe^t}\right)^r[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Hypergeometric Distribution PMF","[$]P(X = k | N, K, n) = \frac{\binom{K}{k} \binom{N-K}{n-k}}{\binom{N}{n}}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Hypergeometric Distribution Mean","[$]\mathbb{E}[X] = \frac{nK}{N}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Hypergeometric Distribution Variance","[$]Var(X) = \frac{nK(N-K)(N-n)}{N^2(N-1)}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Hypergeometric Distribution MGF","???","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Poisson Distribution PMF","[$]P(X = k | \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Poisson Distribution Mean","[$]\mathbb{E}[X] = \lambda[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Poisson Distribution Variance","[$]Var(X) = \lambda[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Poisson Distribution MGF","[$]\mathbb{E}[e^{tX}] = \exp{\left(\lambda(e^t - 1)\right)}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Exponential Distribution PDF","[$]f_X(x | \lambda) = \lambda e^{-\lambda x}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Exponential Distribution Mean","[$]\mathbb{E}[X] = \frac{1}{\lambda}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Exponential Distribution Variance","[$]Var(X) = \frac{1}{\lambda^2}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Exponential Distribution MGF","[$]\mathbb{E}[e^{tX}] = \frac{\lambda}{\lambda - t}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Uniform Distribution PDF","[$]f_X(x) = \begin{cases} \frac{1}{b - a} & a \leq x \leq b\\ 0 & else\\ \end{cases}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Uniform Distribution Mean","[$]\mathbb{E}[X] = \frac{a+b}{2}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Uniform Distribution Variance","[$]Var(X) = \frac{(a-b)^2}{12}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Uniform Distribution MGF","[$]\mathbb{E}[e^{tX}] = \frac{e^{tb} - e^{ta}}{t(b-a)}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Normal Distribution PDF","[$]f_X(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp{\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Normal Distribution Mean","[$]\mathbb{E}[X] = \mu[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Normal Distribution Variance","[$]Var(X) = \sigma^2[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Normal Distribution MGF","[$]\mathbb{E}[e^{tX}] = \exp{\left(\mu t + \frac{\sigma^2 t^2}{2}\right)}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Standard Normal Distribution PDF","[$]f_X(x) = \frac{1}{\sqrt{2\pi}}\exp{\left(-\frac{x^2}{2}\right)}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Standard Normal Distribution Mean","[$]\mathbb{E}[X] = 0[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Standard Normal Distribution Variance","[$]Var(X) = 1[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Standard Normal Distribution MGF","[$]\mathbb{E}[e^{tX}] = \exp{\left(\frac{t^2}{2}\right)}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Chi Squared Distribution PDF","[$]f_X(x | k) = \frac{1}{2^{\frac{k}{2}}\Gamma\left(\frac{k}{2}\right)} x^{\frac{k}{2}-1} e^{-\frac{x}{2}}[/$]  then [$]X \sim \chi^2_k[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Chi Squared Distribution Mean","[$]\mathbb{E}[X] = k[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Chi Squared Distribution Variance","[$]Var(X) = 2k[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Chi Squared Distribution MGF","[$]\mathbb{E}[e^{tX}] = (1 - 2t)^{-\frac{k}{2}}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","t distribution construction","[$]\frac{Z}{\sqrt{\chi^2_k / k}} \sim t_k[/$]  where [$]Z \perp \chi^2_k[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Beta Distribution PDF","[$]f_X(x | \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha - 1} (1 - x)^{\beta - 1}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Beta Distribution Mean","[$]\mathbb{E}[X] = \frac{\alpha}{\alpha + \beta}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Beta Distribution Variance","[$]Var(X) = \frac{\alpha \beta}{(\alpha + \beta + 1)(\alpha + \beta)^2}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Gamma Distribution PDF","[$]f_X(x | \alpha, \beta) = \frac{\beta^{\alpha} }{\Gamma(\alpha)}x^{\alpha - 1} e^{-\beta x}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Gamma Distribution Mean","[$]\mathbb{E}[X] = \frac{\alpha}{\beta}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Gamma Distribution Variance","[$]Var(X) = \frac{\alpha}{\beta^2}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","Gamma Distribution MGF","[$]\mathbb{E}[e^{tX}] = \left(1 - \frac{t}{\beta}\right)^{-\alpha}[/$]","1_Intro_Stats"
"01 First Year Statistics","h Distributions","F distribution setup","[$]\frac{\chi^2_p / p}{\chi^2_q / q} \sim F_{p, q}[/$]  where [$] \chi^2_p \perp \chi^2_q [/$]","1_Intro_Stats"
"02 Linear Algebra","a Vectors","Hadamard Product","Elementwise product of two vectors  [$]\mathbf{x} \circ \mathbf{y}  = \begin{bmatrix}         x_1 y_1\\         x_2 y_2\\         \vdots\\         x_n y_n\\     \end{bmatrix}[/$]","2_Linear_Algebra"
"02 Linear Algebra","a Vectors","Orthogonal Vectors","Vectors whose dot product equals zero  [$]\mathbf{x'y} = 0 [/$]","2_Linear_Algebra"
"02 Linear Algebra","a Vectors","Triangle Inequality","[$]||\mathbf{x} + \mathbf{y}|| \leq ||\mathbf{x}|| + ||\mathbf{y}||[/$]","2_Linear_Algebra"
"02 Linear Algebra","a Vectors","Vector Addition and Subtraction","[$]\mathbf{x} + \mathbf{y} = \begin{bmatrix} x_1 + y_1\\ x_2 + y_2\\ \vdots\\ x_n + y_n\\ \end{bmatrix}, \mathbf{x} - \mathbf{y} = \begin{bmatrix} x_1 - y_1\\ x_2 - y_2\\ \vdots\\ x_n - y_n\\ \end{bmatrix}[/$]",""
"02 Linear Algebra","a Vectors","Unit Vectors [$]\mathbf{e}_i[/$]","The ith entry is one, and the rest are zero:  [$] \mathbf{e}_1 = \begin{bmatrix} 1\\ 0\\ \vdots\\ 0\\ \end{bmatrix}[/$]",""
"02 Linear Algebra","a Vectors","Scalar-Vector Multiplication","[$]\alpha \mathbf{x} = \begin{bmatrix} \alpha x_1\\ \alpha x_2\\ \vdots\\ \alpha x_n\\ \end{bmatrix}[/$]",""
"02 Linear Algebra","a Vectors","Dot Product (Inner Product) of two vectors","[$]\mathbf{x}'\mathbf{y} = \sum_{i=1}^n x_i y_i[/$]",""
"02 Linear Algebra","a Vectors","L2 Norm of a vector","[$]||\mathbf{x}||_2 = (\mathbf{x'x})^{\frac{1}{2}} = \sqrt{x_1^2 + \cdots + x_n^2}[/$]",""
"02 Linear Algebra","a Vectors","Cauchy-Schwartz Inequality","[$]|\mathbf{x'y}| \leq ||\mathbf{x}|| \cdot ||\mathbf{y}||[/$]",""
"02 Linear Algebra","a Vectors","Angle between two vectors","[$]\theta = \arccos{\left(\frac{\mathbf{x'y}}{||\mathbf{x}|| \cdot ||\mathbf{y}||}\right)}[/$]",""
"02 Linear Algebra","b Linear Independence","Linear Dependence","Describes a set of vectors such that   [$]r_1 \mathbf{a}_1 + \cdots + r_k \mathbf{a}_k = \mathbf{0}[/$]  for nonzero constants [$]r_i[/$]",""
"02 Linear Algebra","b Linear Independence","Linear Independence","Describes a set of vectors [$]\mathbf{a}_1, \ldots, \mathbf{a}_k[/$] where  [$]r_1 \mathbf{a}_1 + \cdots + r_k \mathbf{a}_k = 0[/$]  implies that [$]r_1 = \cdots = r_k = 0[/$]",""
"02 Linear Algebra","b Linear Independence","Basis","A set of [$]n[/$] linearly independent vectors in the space [$]\mathbb{R}^n[/$]",""
"02 Linear Algebra","b Linear Independence","Orthonormal Basis","A basis whose vectors are mutually orthogonal and each have length 1",""
"02 Linear Algebra","b Linear Independence","Purpose of Gram-Schmidt","Produce a set of orthonormal vectors [$]\mathbf{q}_i[/$] from a set of vectors [$]\mathbf{a}_i[/$]",""
"02 Linear Algebra","b Linear Independence","Gram-Schmidt Algorithm","1) Normalize the ith vector 2) Subtract all previous projections from the i+1th vector 3) Normalize the difference. This is our i+1th orthonormal basis vector 4) Repeat",""
"02 Linear Algebra","b Linear Independence","QR Factorization","[$]\mathbf{A} = \mathbf{QR}[/$]  where [$]\mathbf{Q}[/$] is an orthonormal matrix and [$]\mathbf{R}[/$] is a upper triangular matrix.  This is computed from Gram-Schmidt",""
"02 Linear Algebra","b Linear Independence","For vectors of order n, what is the maximum amount of linearly independent vectors?  [Independence-dimension Inequality]","There is at most n linearly independent vectors",""
"02 Linear Algebra","b Linear Independence","Basis Expansion","Any vector can ALWAYS be expressed as a UNIQUE linear combination of vectors from a basis.",""
"02 Linear Algebra","b Linear Independence","Orthonormal Basis Expansion","For orthonormal basis [$]\mathbf{a}_1, \ldots, \mathbf{a}_n[/$] and vector [$]\mathbf{x}[/$]:  [$]\mathbf{x} = (\mathbf{a}_1'\mathbf{x})\mathbf{a}_1 + \cdots + (\mathbf{a}_n'\mathbf{x})\mathbf{a}_n[/$]",""
"02 Linear Algebra","c Matrices","Symmetric Matrix","A square matrix where [$]a_{ij} = a_{ji}[/$]",""
"02 Linear Algebra","c Matrices","Diagonal Matrix","A square matrix where [$]a_{ij} = 0[/$] for all [$]i \neq j[/$]",""
"02 Linear Algebra","c Matrices","Matrix Transpose","Denoted as [$]\mathbf{A}'[/$] or [$]\mathbf{A}^{\top}[/$], we have  [$][\mathbf{A}]_{ij} = [\mathbf{A}']_{ji}[/$]",""
"02 Linear Algebra","c Matrices","Matrix Transpose Properties:  [$](\beta \mathbf{A})' = ?[/$]  [$](\mathbf{A} + \mathbf{B})' = ?[/$]  [$](\mathbf{AB})' = ?[/$]  [$]\begin{bmatrix} \mathbf{A} & \mathbf{B}\\ \mathbf{C} & \mathbf{D}\\ \end{bmatrix}' = ?[/$]","[$](\beta \mathbf{A})' = \beta \mathbf{A}'[/$]  [$](\mathbf{A} + \mathbf{B})' = \mathbf{A}' + \mathbf{B}'[/$]  [$](\mathbf{AB})' = \mathbf{B}'\mathbf{A}'[/$]  [$]\begin{bmatrix} \mathbf{A} & \mathbf{B}\\ \mathbf{C} & \mathbf{D}\\ \end{bmatrix}' = \begin{bmatrix} \mathbf{A}' & \mathbf{C}'\\ \mathbf{B}' & \mathbf{D}'\\ \end{bmatrix}[/$]",""
"02 Linear Algebra","c Matrices","Column and row interpretation of [$]\mathbf{AB}[/$]","[$]\mathbf{AB}[/$] is a resuffling of the columns of [$]\mathbf{A}[/$] and a reshuffling of the rows of [$]\mathbf{B}[/$]",""
"02 Linear Algebra","c Matrices","Lower Triangular Matrix","A square matrix [$]\mathbf{A}[/$] where  [$]a_{ij} = 0[/$] for all [$] i < j[/$]",""
"02 Linear Algebra","c Matrices","Upper Triangular Matrix","A square matrix [$]\mathbf{A}[/$] where  [$]a_{ij} = 0[/$] for all [$] i > j[/$]",""
"02 Linear Algebra","c Matrices","Unit Upper Triangular Matrix","A square matrix [$]\mathbf{A}[/$] where  [$]a_{ij} = 0[/$] for all [$] i > j[/$] and [$]a_{ii} = 1[/$]",""
"02 Linear Algebra","c Matrices","Unit Lower Triangular Matrix","A square matrix [$]\mathbf{A}[/$] where  [$]a_{ij} = 0[/$] for all [$] i < j[/$] and [$]a_{ii} = 1[/$]",""
"02 Linear Algebra","c Matrices","Scalar-Matrix Multiplication","[$]\beta \mathbf{A} = \begin{bmatrix} \beta a_{11} & \cdots & \beta a_{1n}\\ \vdots & \ddots & \vdots\\ \beta a_{m1} & \cdots & \beta a_{mn}\\ \end{bmatrix}[/$]",""
"02 Linear Algebra","c Matrices","Matrix Addition and Subtraction","[$] \mathbf{A} + \mathbf{B} = \begin{bmatrix} a_{11} + b_{11} & \cdots & a_{1n} + b_{1n}\\ \vdots & \ddots & \vdots\\  a_{m1} + b_{m1} & \cdots & a_{mn} + b_{mn}\\ \end{bmatrix}[/$]  [$] \mathbf{A} - \mathbf{B} = \begin{bmatrix} a_{11} - b_{11} & \cdots & a_{1n} - b_{1n}\\ \vdots & \ddots & \vdots\\  a_{m1} - b_{m1} & \cdots & a_{mn} - b_{mn}\\ \end{bmatrix}[/$]",""
"02 Linear Algebra","c Matrices","Matrix Norm Properties","Positive Definiteness: [$]||\mathbf{A}|| = \mathbf{0}[/$] if and only if [$]\mathbf{A} = \mathbf{0}[/$]  Homogeneity: [$]||\alpha \mathbf{A}|| = |\alpha| \cdot ||\mathbf{A}||[/$] for any scalar [$]\alpha[/$]  Triangle Inequality: [$]||\mathbf{A} + \mathbf{B}|| \leq ||\mathbf{A}|| + ||\mathbf{B}||[/$]  Cauchy Schwartz: [$]||\mathbf{AB}|| \leq ||\mathbf{A}|| \cdot ||\mathbf{B}||[/$]",""
"02 Linear Algebra","c Matrices","Gram Matrix","For any matrix [$]\mathbf{A}[/$], its associated Gram Matrix is  [$]\mathbf{A'A}[/$]",""
"02 Linear Algebra","c Matrices","If the columns of [$]\mathbf{A}[/$] are linearly independent,  what does that say about [$]\mathbf{Ab} = \mathbf{0}[/$]?","It implies that [$]\mathbf{b} = \mathbf{0}[/$]",""
"02 Linear Algebra","d Subspaces","Vector Space","A set of vectors is a vector space if:  1) Closed under addition 2) Closed under scalar multiplication 3) The existence of a zero vector",""
"02 Linear Algebra","d Subspaces","Order of a vector space","The size of a vector in a vector space  (similar to dimension)",""
"02 Linear Algebra","d Subspaces","Span","The span of a set of vectors [$]\mathbf{x}_1, \ldots, \mathbf{x}_k[/$] is the set of all linear combinations of [$]\mathbf{x}_i[/$]'s",""
"02 Linear Algebra","d Subspaces","Dimension of a subspace, dim[$](\mathcal{S})[/$]","The number of vectors in the basis of [$]\mathcal{S}[/$].",""
"02 Linear Algebra","d Subspaces","Column Space","Span of the columns of [$]\mathbf{A}[/$]",""
"02 Linear Algebra","d Subspaces","Row Space","Span of the rows of [$]\mathbf{A}[/$].   [$]\mathcal{R}(\mathbf{A}) = \mathcal{C}(\mathbf{A}')[/$]  [$] = \{y \in \mathbb{R}^n : \mathbf{y} = \mathbf{A}'\mathbf{x}[/$] for some [$]\mathbf{x} \in \mathbb{R}^m\}[/$]",""
"02 Linear Algebra","d Subspaces","Null Space","Set of vectors such that [$]\mathbf{Ax = 0}[/$]  [$]\mathcal{N}(\mathbf{A}) = \{\mathbf{x} \in \mathbb{R}^n : \mathbf{Ax = 0}[/$]",""
"02 Linear Algebra","d Subspaces","Left Null Space","[$]\mathcal{N}(\mathbf{A}') = \{\mathbf{x}\in \mathbb{R}^m : \mathbf{A}'\mathbf{x} = \mathbf{0}\}[/$]",""
"02 Linear Algebra","d Subspaces","What are the four fundamental subspaces?  Which of them are disjoint?","Column space Row space Null space Left null space  Row space and null space are disjoint Column space and left null space are disjoint",""
"02 Linear Algebra","d Subspaces","Disjoint Subspaces","Two subspaces [$]\mathcal{S}_1[/$] and [$]\mathcal{S}_2[/$] are disjoint if   [$]\mathcal{S}_1 \cap \mathcal{S}_2 = \emptyset[/$]",""
"02 Linear Algebra","d Subspaces","How does matrix multiplication affect column space?  Namely, if [$]\mathbf{C} = \mathbf{AB}[/$], how does the column space of [$]\mathbf{A}[/$] compare to [$]\mathbf{C}[/$]?","[$]\mathcal{C}(\mathbf{C}) \subseteq \mathcal{C}(\mathbf{A})[/$]",""
"02 Linear Algebra","d Subspaces","How does matrix multiplication affect row space?  Namely, if [$]\mathbf{C} = \mathbf{AB}[/$], how does the column space of [$]\mathbf{B}[/$] compare to [$]\mathbf{C}[/$]?","[$]\mathcal{R}(\mathbf{C}) \subseteq \mathcal{R}(\mathbf{B})[/$]",""
"02 Linear Algebra","d Subspaces","How does matrix multiplication affect null space?  Namely, if [$]\mathbf{C} = \mathbf{AB}[/$], how does the null space of [$]\mathbf{B}[/$] compare to [$]\mathbf{C}[/$]?","[$]\mathcal{N}(\mathbf{B}) \subseteq \mathcal{N}(\mathbf{C})[/$]",""
"02 Linear Algebra","d Subspaces","What is the null space of a Gram Matrix?  [$]\mathcal{N}(\mathbf{A'A}) = ?[/$]","[$]\mathcal{N}(\mathbf{A'A}) = \mathcal{N}(\mathbf{A})[/$]",""
"02 Linear Algebra","d Subspaces","Basis of a subspace","A set of linearly independent vectors that span a vector space [$]\mathcal{S}[/$]",""
"02 Linear Algebra","e Rank and Nullity","How does rank[$](\mathbf{AB})[/$] relate to rank[$](\mathbf{A})[/$] and rank[$](\mathbf{B})[/$]?","[$]rank(\mathbf{AB}) \leq min(rank(\mathbf{A}), rank(\mathbf{B}))[/$]","2_Linear_Algebra"
"02 Linear Algebra","e Rank and Nullity","Rank of a matrix","The rank of [$]\mathbf{A}[/$] is the number of linearly independent columns of [$]\mathbf{A}[/$].  [$]\text{rank}(\mathbf{A}) = \text{dim}(\mathcal{C}(A))[/$]",""
"02 Linear Algebra","e Rank and Nullity","Rank nullity theorem","Rank + null of a matrix equals the number of columns.  For [$]m \times n[/$] matrix [$]\mathbf{A}[/$], [$]\text{rank}(\mathbf{A}) + \text{null}(\mathbf{A}) = n[/$]",""
"02 Linear Algebra","e Rank and Nullity","How does rank[$](\mathbf{AB})[/$] compare to rank[$](\mathbf{A})[/$] and rank[$](\mathbf{B})[/$]?","[$]\text{rank}(\mathbf{AB}) \leq \text{min} (\text{rank}(\mathbf{A}), \text{rank}(\mathbf{B})[/$]",""
"02 Linear Algebra","e Rank and Nullity","Prove that  [$]\text{rank}(\mathbf{A}) = \text{rank}(\mathbf{A'}) = \text{rank}(\mathbf{A'A}) = \text{rank}(\mathbf{AA'})[/$]","",""
"02 Linear Algebra","e Rank and Nullity","Prove that column rank equals row rank","Rank factorization theorem",""
"02 Linear Algebra","e Rank and Nullity","What does it mean for a matrix to be full rank?","When [$]\text{rank}(\mathbf{A}) = \text{min}(m,n)[/$]",""
"02 Linear Algebra","e Rank and Nullity","What effect does matrix multiplication have on rank?  Namely, how do [$]\text{rank}(\mathbf{A})[/$] and [$]\text{rank}(\mathbf{AB})[/$] compare?","[$]\text{rank}(\mathbf{AB}) \leq \text{rank}(\mathbf{A})[/$]",""
"02 Linear Algebra","e Rank and Nullity","When does   [$]\text{rank}(\mathbf{AB}) = \text{rank}(\mathbf{A})[/$]?  [$]\text{rank}(\mathbf{AB}) = \text{rank}(\mathbf{B})[/$]?","[$]\text{rank}(\mathbf{AB}) = \text{rank}(\mathbf{A})[/$] when [$]\mathbf{B}[/$] has full row rank  [$]\text{rank}(\mathbf{AB}) = \text{rank}(\mathbf{B})[/$] when [$]\mathbf{A}[/$] has full column rank",""
"02 Linear Algebra","e Rank and Nullity","Rank Factorization of [$]\mathbf{A} \in \mathbb{R}^{m \times n}[/$]  where [$]\mathbf{A}[/$] has rank r.","[$]\mathbf{A} = \mathbf{CR}[/$] where  [$]\mathbf{C} \in \mathbb{R}^{m \times r}[/$]  [$]\mathbf{R} \in \mathbb{R}^{r \times n}[/$]",""
"02 Linear Algebra","f Inverse","Definition of matrix inverse","Matrix [$]\mathbf{A}[/$] has an inverse [$] \mathbf{A}^{-1}[/$] when  [$]\mathbf{AA}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}[/$]",""
"02 Linear Algebra","f Inverse","When does a matrix have an inverse?","It is a square matrix with full rank.",""
"02 Linear Algebra","f Inverse","Definition of left inverse","[$] \mathbf{B}[/$] is the left inverse of [$]\mathbf{A}[/$] if   [$] \mathbf{BA} = \mathbf{I}[/$]",""
"02 Linear Algebra","f Inverse","Definition of right inverse","[$] \mathbf{B}[/$] is the right inverse of [$]\mathbf{A}[/$] if   [$] \mathbf{AB} = \mathbf{I}[/$]",""
"02 Linear Algebra","f Inverse","What is the inverse of an orthonormal matrix?","[$]\mathbf{A}^{-1} = \mathbf{A}'[/$]",""
"02 Linear Algebra","f Inverse","Prove that  [$](\mathbf{A}')^{-1} = (\mathbf{A}^{-1})'[/$]","[$](\mathbf{A}')^{-1} \mathbf{A}' = \mathbf{I} = \left[(\mathbf{A}')^{-1} \mathbf{A}'\right]'[/$]  [$] = \mathbf{A} (\mathbf{A}')^{-1}'[/$]  Then, we get that  [$] (\mathbf{A}')^{-1}' = \mathbf{A}^{-1}[/$]  Finally, invert both sides to get  [$] (\mathbf{A}')^{-1} = (\mathbf{A}^{-1})'[/$]",""
"02 Linear Algebra","f Inverse","What is the inverse of a matrix product?  [$] (\mathbf{AB})^{-1} = ?[/$]","[$] (\mathbf{AB})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}[/$]",""
"02 Linear Algebra","f Inverse","[OVERLY DETERMINED SYSTEMS]  For full rank tall matrix [$]\mathbf{A}[/$], it has infinitely many left inverses, [$]\mathbf{X}[/$]:  [$]\mathbf{Ax} = \mathbf{b}[/$]  [$] \mathbf{XAx} = \mathbf{Xb}[/$]  [$] \mathbf{x} = \mathbf{Xb}[/$]  Regardless of what left inverse we use, [$]\mathbf{Xb}[/$] will always give us the same solution. Why? Prove it.","??",""
"02 Linear Algebra","f Inverse","LU Decomposition  1) What is it? 2) What is the algorithm to do it?","1) For any nonsingular square matrix [$]\mathbf{A}[/$], we can express it as   [$]\mathbf{A} = \mathbf{LU}[/$]   where [$] \mathbf{L}[/$] is unit lower triangular and [$]\mathbf{U}[/$] is upper traingular.   2) Algorithmically, it is achived from Gaussian Elimination.",""
"02 Linear Algebra","f Inverse","Generalized Inverse","[$]\mathbf{G}[/$] is a generalized inverse of a matrix [$]\mathbf{A}[/$] if it satisfies the following:  [$]\mathbf{AGA} = \mathbf{A}[/$]",""
"02 Linear Algebra","f Inverse","Moore-Penrose Inverse","[$]\mathbf{G}[/$] is the Moore-Penrose Inverse of [$]\mathbf{A}[/$] if it satisfies the following:  [$]\mathbf{AGA} = \mathbf{A}[/$]  [$]\mathbf{GAG} = \mathbf{G}[/$]  [$](\mathbf{AG})' = \mathbf{AG}[/$]  [$](\mathbf{GA})' = \mathbf{GA}[/$]  We denote the Moore-Penrose Inverse as [$]\mathbf{A}^+[/$] '",""
"02 Linear Algebra","g Orthogonal Projections","Dimension of a sum of subspaces","[$]\text{dim}(\mathcal{S}_1 + \mathcal{S}_2) = \text{dim}(\mathcal{S}_1) + \text{dim}(\mathcal{S}_2) - \text{dim}(\mathcal{S}_1 \cap \mathcal{S}_2)[/$]",""
"02 Linear Algebra","g Orthogonal Projections","Complementary Subspaces","Two subspaces [$]\mathcal{S}_1[/$] and [$]\mathcal{S}_2[/$] are complimentary when  [$]\mathcal{S}_1 \cap \mathcal{S}_2 = \{\textbf{0}\}[/$]",""
"02 Linear Algebra","g Orthogonal Projections","Direct Sum of subspaces: [$]\oplus [/$]","A direct sum can be performed on two subspaces when they are complimentary. They form a new subspace.  [$]\mathcal{V} = \mathcal{S}_1 \oplus \mathcal{S}_2[/$]",""
"02 Linear Algebra","g Orthogonal Projections","Unique decomposition property of direct sums.  [$]\mathcal{V} = \mathcal{S}_1 \oplus \mathcal{S}_2[/$]","Any vector [$]\mathbf{x} \in \mathcal{V}[/$] can be uniquely expressed as   [$]\mathbf{x} = \mathbf{x}_1 + \mathbf{x}_2[/$]   where   [$]\mathbf{x}_1 \in \mathcal{S}_1[/$] and [$]\mathbf{x}_2 \in \mathcal{S}_2[/$]",""
"02 Linear Algebra","g Orthogonal Projections","Orthocomplement set of a set [$]\mathcal{X}[/$]","[$]\mathcal{X}^{\perp} = \{\mathbf{u} \in \mathcal{V} : \langle \mathbf{x}, \mathbf{u} \rangle =  0 \text{for all} \mathbf{x} \in \mathcal{X}\}[/$]",""
"02 Linear Algebra","g Orthogonal Projections","Direct sum theorem for orthocomplementary subspaces  Let [$]\mathcal{S}[/$] be a subspace of vector space [$]\mathcal{V}[/$].   What can we say about [$]\mathcal{S}[/$] and [$]\mathcal{S}^{\perp}[/$]?","[$]\mathcal{V} = \mathcal{S} \oplus \mathcal{S}^{\perp}[/$] ------------------------- Every vector [/$]\mathbf{x} \in \mathcal{V}[/$] can be uniquely expressed as [$]\mathbf{x} = \mathbf{u} + \mathbf{v}[/$] where:  [$] \mathbf{u} \in \mathcal{S} \text{ and } \mathbf{v} \in \mathcal{S}[/$] ----------- [$]\mathcal{S} \cap \mathcal{S}^{\perp} = \{\textbf{0}\}[/$]",""
"02 Linear Algebra","g Orthogonal Projections","Fundamental Theorem of Linear Algebra  How do the four fundamental subspaces relate to each other?","Let [$]\mathbf{A} \in \mathbb{R}^{m \times n}[/$]  [$]\mathcal{C}(\mathbf{A})^{\perp} = \mathcal{N}(\mathbf{A}')[/$] and therefore [$]\mathcal{C}(\mathbf{A}) \oplus \mathcal{N}(\mathbf{A}') = \mathbf{R}^m[/$]  [$]\mathcal{C}(\mathbf{A}') = \mathcal{N}(\mathbf{A})^{\perp}[/$] and therefore [$]\mathcal{C}(\mathbf{A}') \oplus \mathcal{N}(\mathbf{A}) = \mathbf{R}^n[/$]",""
"02 Linear Algebra","g Orthogonal Projections","What is an orthogonal projection?  Let [$] \mathbf{y} \in \mathcal{V}[/$]. What is the orthogonal projection of [$]\mathbf{y}[/$] into a subspace [$]\mathcal{S}[/$]?","Let [$]\mathbf{u} \in \mathcal{S}[/$] be the orthogonal projection of [$]\mathbf{y}[/$] into [$]\mathcal{S}[/$].  Then:  [$]||\mathbf{y} - \mathbf{u}||^2 \leq ||\mathbf{y} - \mathbf{w}||^2[/$]  for all [$]\mathbf{w} \in \mathcal{S}[/$].",""
"02 Linear Algebra","g Orthogonal Projections","Orthogonal Projection Matrix","For vector [$]\mathbf{y}[/$] and subspace [$]\mathcal{S}[/$], the orthogonal projection matrix [$]\mathbf{P}[/$] relates:  [$] \mathbf{Py} = \mathbf{u}[/$]  where [$]\mathbf{u}[/$] is the orthogonal projection of [$]\mathbf{y}[/$] into [$]\mathcal{S}[/$].",""
"02 Linear Algebra","g Orthogonal Projections","Prove that the orthogonal projection matrix exists and is unique.","???",""
"02 Linear Algebra","g Orthogonal Projections","Construction of orthogonal projection matrix.  Start with an orthonormal basis [$]\mathbf{q_1}, \ldots, \mathbf{q}_r[/$] for subspace [$]\mathcal{S}[/$].","[$]\mathbf{P} = \mathbf{QQ'}[/$]   where  [$]\mathbf{Q}[/$] is the matrix consisting of column vectors of the basis.  OR  [$]\mathbf{P} = \mathbf{X}(\mathbf{X'X})^- \mathbf{X}'[/$]",""
"02 Linear Algebra","g Orthogonal Projections","Prove that any symmetric, idempotent matrix [$]\mathbf{P}[/$] is the orthogonal projector into [$]\mathcal{C}(\mathbf{P})[/$]","For any [$]\mathbf{x} \in \mathbb{R}^n[/$], let [$]\mathbf{x} = \mathbf{v} + \mathbf{w}[/$] where [$]\mathbf{v} \in \mathcal{C}(\mathbf{P})[/$] and   [$]\mathbf{w} \in \mathcal{C}(\mathbf{P})^{\perp} = \mathcal{N}(\mathbf{P}') = \mathcal{N}(\mathbf{P})[/$]  Then, we see that [$]\mathbf{v} = \mathbf{Pu}[/$] for some u.  Then, [$] \mathbf{Px} = \mathbf{Pv} + \mathbf{Pw} = \mathbf{PPu} + \mathbf{Pw} = \mathbf{Pu} + \mathbf{Pw} = \mathbf{v} + 0 = \mathbf{v}[/$]  Thus, [$]\mathbf{P}[/$] is the orthogonal projector into [$]\mathcal{C}(\mathbf{P})[/$].",""
"02 Linear Algebra",NA,"What are [$]\sum_{i=1}^n \lambda_i[/$] and [$]\prod_{i=1}^n \lambda_i[/$]?","[$] tr(\mathbf{A}) = \sum_{i=1}^n \lambda_i[/$]  [$] det(\mathbf{A}) = \prod_{i=1}^n \lambda_i [/$]","2_Linear_Algebra"
"02 Linear Algebra",NA,"What is the spectral decomposition of a symmetric [$]n \times n[/$] matrix [$]\mathbf{A}[/$]","[$] \mathbf{A} = T \boldsymbol{\Lambda} T'[/$]","2_Linear_Algebra"
"02 Linear Algebra",NA,"What are the eigenvalues of [$]\mathbf{A}^{-1}[/$]?","If the eigenvalues of [$]\mathbf{A} = \lambda_i[/$],  then the eigenvalues of [$]\mathbf{A}^{-1} = \lambda_i^{-1}[/$]","2_Linear_Algebra"
"3 Multivariate Statistics (250A)","a Random Vectors","Expectation of a random vector","Expectation of a random vector is just the vector of the individual expectations:  [$]\mathbb{E}[\mathbf{x}] = \begin{bmatrix} \mathbb{E}[x_1]\\ \mathbb{E}[x_2]\\ \vdots\\ \mathbb{E}[x_n]\\ \end{bmatrix}[/$]",""
"3 Multivariate Statistics (250A)","a Random Vectors","Expectation of a linear combination of a random matrix.  [$]\mathbb{E}[\mathbf{AZB} + \mathbf{C}] = ?[/$]  where [$]\mathbf{A}, \mathbf{B}, \mathbf{C}[/$] are constant matrices and [$]\mathbf{Z}[/$] is a random matrix.","[$]\mathbb{E}[\mathbf{AZB} + \mathbf{C}] = \mathbf{A} \mathbb{E}[\mathbf{Z}] \mathbf{Z} + \mathbf{C}[/$]",""
"3 Multivariate Statistics (250A)","a Random Vectors","Covariance of two random vectors.  [$]\text{Cov}(\mathbf{x}, \mathbf{y}) = ?[/$]","[$]\text{Cov}(\mathbf{x}, \mathbf{y}) = (\text{cov}[x_i, y_j])[/$]  More colloquially, we get a ""covariance matrix"" that describes the individual covariances between entries of the random vectors.",""
"3 Multivariate Statistics (250A)","a Random Vectors","Variance of a random vector.  [$]\text{Var}(\mathbf{x}, \mathbf{x}) = ?[/$]","[$]\text{Var}(\mathbf{x}, \mathbf{x}) = \begin{bmatrix} \text{var}(x_1) & cov(x_1, x_2) & \cdots & cov(x_1, x_n)\\ cov(x_2, x_1) & var(x_2) & \cdots & cov(x_2, x_n)\\ \vdots & \vdots & \ddots & \vdots\\ cov(x_n, x_1) & cov(x_n, x_2) & \cdots & var(x_n)\\ \end{bmatrix}[/$]  Also called the covariance matrix of [$]\mathbf{x}[/$]",""
"3 Multivariate Statistics (250A)","a Random Vectors","[$]\text{var}(\mathbf{x} - \mathbf{a}) = ? [/$]","[$]\text{var}(\mathbf{x} - \mathbf{a}) = \text{var}(\mathbf{x})[/$]",""
"3 Multivariate Statistics (250A)","a Random Vectors","[$]\text{Cov}(\mathbf{AX}, \mathbf{BY}) = ? [/$]  where [$]\mathbf{A}, \mathbf{B}[/$] are constant matrices and [$]\mathbf{X}, \mathbf{Y}[/$] are random matrices.","[$]\text{Cov}(\mathbf{AX}, \mathbf{BY}) = \mathbf{A} \text{Cov}(\mathbf{X}, \mathbf{Y}) \mathbf{B}'[/$]",""
"3 Multivariate Statistics (250A)","a Random Vectors","[$]\text{Var}(\mathbf{AX}) = ?[/$]  where [$]\mathbf{A}[/$] is a constant matrix and [$]\mathbf{X}[/$] is a random matrix","[$]\text{Var}(\mathbf{AX}) = \mathbf{A} \text{Var}(\mathbf{X}) \mathbf{A}'[/$]",""
"3 Multivariate Statistics (250A)","a Random Vectors","[$]\text{Cov}(a \mathbf{X} + b \mathbf{Y}, c \mathbf{U} + d \mathbf{V} = ? [/$]","[$]\text{Cov}(a \mathbf{X} + b \mathbf{Y}, c \mathbf{U} + d \mathbf{V} =  ac \text{Cov}(\mathbf{X}, \mathbf{U}) + ad \text{Cov}(\mathbf{X}, \mathbf{V}) + bc \text{Cov}(\mathbf{Y}, \mathbf{U}) + bd \text{Cov}(\mathbf{Y}, \mathbf{V})[/$]",""
"3 Multivariate Statistics (250A)","a Random Vectors","[$]\text{Var}(a \mathbf{X} + b \mathbf{Y}) = ?[/$]","[$]\text{Var}(a \mathbf{X} + b \mathbf{Y}) = a^2 \text{Var}(\mathbf{X}) + 2ab \cov{(\mathbf{X}, \mathbf{Y})} + b^2 \var{(\mathbf{Y})}[/$]",""
"3 Multivariate Statistics (250A)","a Random Vectors","Expectation of a quadratic form:  [$]\mathbb{E}[\mathbf{x}' \mathbf{Ax}] = ?[/$]","[$]\mathbb{E}[\mathbf{x}' \mathbf{Ax}] = \text{tr}(\mathbf{A} \boldsymbol{\Sigma} + \boldsymbol{\mu}' \mathbf{A} \boldsymbol{\mu}[/$]  where [$]\mathbb{E}[\mathbf{x}] = \boldsymbol{\mu}[/$] and [$]\var{(\mathbf{x})} = \boldsymbol{\Sigma}[/$]",""
"3 Multivariate Statistics (250A)","a Random Vectors","How can moment generating functions prove independence between two random vectors?","If we have random vectors [$]\mathbf{x}_1, \mathbf{x}_2[/$], and we stack them to make [$]\mathbf{x}[/$], then they are independent if we can show that  [$]M_{\mathbf{x}}(\mathbf{t}) = M_{\mathbf{x}}(t_1, \ldots, t_r, 0, \ldots, 0) M_{\mathbf{x}}(0, \ldots, 0, t_{r+1}, \ldots, t_n)[/$]  Or, we can show that the mgf simplifies to:  [$]M_{\mathbf{x}(\mathbf{t}) = a(t_1, \ldots, t_r) \cdot b(t_1, \ldots, t_r)[/$]  for some functions a() and b().",""
"3 Multivariate Statistics (250A)","a Random Vectors","Moment generating function (MGF) for a random vector","[$]M_{\mathbf{x}(\mathbf{t}) = \mathbb{E}\left[\exp{(\mathbf{t'x})}\right][/$]",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","Expectation and variance of a multivariate normal random variable","[$]\mathbb{E}[\mathbf{x}] = \boldsymbol{\mu}[/$]  [$]\var{(\mathbf{x})} = \boldsymbol{\Sigma}[/$]",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","How can we transform a multivariate normal distribution into a standard multivariate normal distribution?  [$]\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})[/$]  How can we obtain [$]\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})[/$]?","[$]\mathbf{z} = \boldsymbol{\Sigma}^{-\frac{1}{2}} (\mathbf{x} - \boldsymbol{\mu}) \sim \mathcal{N}(\mathbf{0}, \mathbf{I})[/$]",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","What is the MGF of  [$]\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})[/$]  and   [$]\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})[/$]","[$]M_{\mathbf{z}}(\mathbf{t}) = \exp{(\frac{1}{2}\mathbf{t't})}[/$]  [$]M_{\mathbf{x}}(\mathbf{t}) = \exp{(\mathbf{t}' \boldsymbol{\mu} + \frac{1}{2} \mathbf{t}' \boldsymbol{\Sigma} \mathbf{t})}[/$]",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","Let [$]\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})[/$] and [$]\mathbf{C}, \mathbf{d}[/$] be a constant matrix and a constant vector respectively.   [$]\mathbf{C}\mathbf{x} + \mathbf{d} \sim ?[/$]","[$]\mathbf{C}\mathbf{x} + \mathbf{d} \sim \mathcal{N}(\mathbf{C} \boldsymbol{\mu} + \mathbf{d}, \mathbf{C} \boldsymbol{\Sigma} \mathbf{C}')[/$]",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","How are two multivariate normal distributions independent?","Two multivariate normal distributions are independent if and only if their covariance matrix is 0.  The proof involves the MGF.",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","Let [$]\mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})[/$]   Let [$]\mathbf{u} = \mathbf{Ay}[/$] and [$]\mathbf{v} = \mathbf{By}[/$].  When is [$]\mathbf{u} \perp \mathbf{v}[/$]?","[$]\mathbf{u} \perp \mathbf{v}[/$]  if and only if  [$]\cov{(\mathbf{u}, \mathbf{v})} = \mathbf{A} \boldsymbol{\Sigma} \mathbf{B}' = 0[/$]",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","Prove that if [$]\mathbf{y} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),   and if [$]\mathbf{A}[/$] is a symmetric matrix  then [$]\mathbf{y}' \mathbf{Ay} \sim \chi^2_r [/$] if and only if [$]\mathbf{A}[/$] is idempotent with rank r.","We utilize the property of idempotent matrices. An idempotent matrix of rank r must have r eigenvalues equal to 1 and the rest 0. Then:  [$]\mathbf{T'AT} = \mathbf{D}[/$] with r 1's and the rest 0. Then:  [$]\mathbf{y'Ay} = \mathbf{y'TDT'y} = \mathbf{z'Dz} = \sum_{i=1}^n d_i [/$]  ???",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","Show that   [$]\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}[/$]  for a sample of n iid normally distributed observations","???",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","Hogg Craig Theorem","???",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","If [$]\mathbf{y} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Sigma})[/$]   when is [$]\mathbf{y'Ay} \sim \chi^2_{r}[/$]?","[$]\mathbf{y'Ay} \sim \chi^2_{r}[/$]  if and only if   [$]\mathbf{A}\boldsymbol{\Sigma}[/$] has r eigenvalues equal to 1 and the rest are 0.  [Theorem 2.8]",""
"3 Multivariate Statistics (250A)","b Multivariate Normal Distribution","Let [$]\mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})[/$]   prove [$]Q = (\mathbf{y} - \boldsymbol{\mu})' \boldsymbol{\Sigma}^{-1} (\mathbf{y} - \boldsymbol{\mu}) \sim \chi^2_n[/$]","Let [$]\mathbf{y} = \boldsymbol{\Sigma}^{\frac{1}{2}} \mathbf{z} + \boldsymbol{\mu}[/$] for standard multivariate normal random variable z.  Then, we get   [$]Q = \mathbf{z}' \boldsymbol{\Sigma}^{\frac{1}{2}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\Sigma}^{\frac{1}{2}} \mathbf{z} = \mathbf{z'z} = \sum_{i=1}^n z_i^2[/$]  and since the [$]z_i[/$]'s are independent [$]\chi^2_1[/$] variables, [$]Q \sim \chi^2_n[/$]",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","Write the OLS model","[$]\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}[/$]  where [$]\varepsilon_i \sim \mathcal{N}(0, \sigma^2)[/$] are iid",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","Write the normal equations for least squares   interpret them","[$]\mathbf{X'X} \boldsymbol{\beta} = \mathbf{X}' \mathbf{y}[/$]  --------------------------------------------------------------------------------------  Let [$]\boldsymbol{\theta}[/$] be the best estimate of [$]\mathbf{y}[/$] in the subspace that can be achieved by [$]\mathbf{X}[/$]. That is:  [$]\boldsymbol{\theta} = \mathbf{X} \boldsymbol{\beta}[/$]. Then we get  [$]\hat{\boldsymbol{\theta}} = \mathbf{Py}, \mathbf{P}\boldsymbol{\theta} = \boldsymbol{\theta}[/$]  Then, notice that [$]\mathbf{y} - \hat{\boldsymbol{\theta}}[/$] is perpentdicular to [$]\mathbf{X}[/$]. Thus:  [$]\mathbf{X}' (\mathbf{y} - \hat{\boldsymbol{\theta}}) = \mathbf{0}[/$]  [$]\mathbf{X}' \hat{\boldsymbol{\theta}} = \mathbf{X'y}[/$]  [$]\mathbf{X'X} \boldsymbol{\beta} = \mathbf{X}' \mathbf{y}[/$]",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","What is the least squares estimate of [$]\hat{\boldsymbol{\beta}}[/$]?","[$]\hat{\boldsymbol{\beta}} = (\mathbf{X'X})^{-1} \mathbf{X'y}[/$]",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","Find the formula for the residuals in OLS. Utilize the projection matrix.  [$]\mathbf{e} = ?[/$]","[$]\mathbf{e} = \mathbf{y} - \hat{\mathbf{y} = \mathbf{y} - \mathbf{X} \boldsymbol{\beta} = (\mathbf{I} - \mathbf{P}) \mathbf{y}[/$]",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","Find the formula for the residual sum of squares (RSS) in OLS. Utilize the projection matrix.  [$]\mathbf{e'e} = ?[/$]","[$]\mathbf{e'e} = (\mathbf{y} - \hat{\mathbf{y}})'  (\mathbf{y} - \hat{\mathbf{y}})[/$]  [$] = (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})' (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})[/$]  [$] = \mathbf{y}'(\mathbf{I} - \mathbf{P}) \mathbf{y}[/$]",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","Prove that [$]\hat{\boldsymbol{\theta}} = \mathbf{X} \hat{\boldsymbol{\beta}}[/$] is unique irrespective of the rank of X.","???  Something to do with over-specified systems having at most one solution if at all.",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","What is the formula for the projection matrix in OLS?  [$]\mathbf{P} = ?[/$]","[$]\mathbf{P} = \mathbf{X}(\mathbf{X'X})^{-1} \mathbf{X}'[/$]",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","If [$]\mathbf{X}[/$] is a full rank design matrix, prove that  [$] \sum_{i=1}^n (y_i - \hat{y}_i) = 0[/$]","[$] \sum_{i=1}^n (y_i - \hat{y}_i) = \mathbf{1}' (\mathbf{y} - \hat{\mathbf{y}})[/$]  However, since [$]\mathbf{1} \in \mathcal{C}(\mathbf{X})[/$], then the residuals are orthgonal to the one vector.  Therefore, their inner product is zero.",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","In OLS, what is the expectation and variance of [$]\hat{\boldsymbol{\beta}}[/$]?","[$]\mathbb{E}[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}[/$]  [$]\var{(\hat{\boldsymbol{\beta}})} = \sigma^2 (\mathbf{X'X})^{-1}[/$]",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","What is BLUE?","Best linear unbiased estimator.  But what does it mean??? ???",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","In OLS, what is the unbiased estimator of [$]\sigma^2[/$]?  Prove that it's unbiased.","[$]S^2 = \frac{RSS}{n-r}[/$]  where [$]\mathbf{X}[/$] is rank r. It doesn't have to be full rank.  Then, from quadratic forms, we see that  [$](n-r)S^2 = \mathbf{y}' (\mathbf{I} - \mathbf{P}) \mathbf{y}[/$]  and from the expectation of quadratic forms, we see that  [$]\mathbb{E}[\mathbf{y}' (\mathbf{I} - \mathbf{P}) \mathbf{y}] = \sigma^2 (n-r)[/$]  and thus [$]S^2[/$] is unbiased",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","For the OLS estimate, [$]\hat{\boldsymbol{\beta}}[/$], find its distribution  We are assuming that the errors are iid standard normal.","[$]\hat{\boldsymbol{\beta} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}'\mathbf{X}){^-1})[/$]",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","In OLS, with the assumption that the error is iid normal, and [$]\mathbf{X}[/$] is full rank, find the distribution of:  [$]\frac{(\hat{\boldsymbol{\beta}})' \mathbf{X'X}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})}{\sigma^2}[/$]","[$]\frac{(\hat{\boldsymbol{\beta}})' \mathbf{X'X}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})}{\sigma^2} \sim \chi^2_p[/$]  where [$]\mathbf{X}[/$] is full rank n x p matrix.",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","In OLS, with the assumption that the errors are iid normal, and that X is full rank, prove that:  [$]\hat{\boldsymbol{\beta}} \perp \S^2[/$]","First show that   [$]\cov{(\hat{\boldsymbol{\beta}}, \mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})} = \mathbf{0}[/$]  Then it follows that [$]\hat{\boldsymbol{\beta}} \perp \S^2[/$]",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","In OLS, with the assumption that the errors are iid normal, and that X is full rank, prove that  [$]\frac{RSS}{\sigma^2} = \frac{(n-p)S^2}{\sigma^2} \sim \chi^2-{n-p}[/$]","Since [$]RSS = \boldsymbol{\varepsilon}' (\mathbf{I} - \mathbf{P}) \boldsymbol{\varepsilon}[/$], we invoke the distribution of quadratic forms",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","Write out the MLE for beta in OLS","???",""
"3 Multivariate Statistics (250A)","c Least Squares Theory","For two design matrices [$]\mathbf{X}_1, \mathbf{X}_2[/$] where \mathcal{C}(\mathbf{X}_1) \subseteq \mathcal{C}(\mathbf{X}_2)[/$],  how do their projection matrices relate?","[$]P_{\mathbf{X}_1} P_{\mathbf{X}_2} = P_{\mathbf{X}_1}[/$]  Their product equals the smaller projection matrix.",""
"4 Coding","a Linux","How to clone a repo with github","git clone [url]",""
"4 Coding","a Linux","How to update your local github repo","git pull",""
"4 Coding","a Linux","How to move a file from your working directory to the remote github repo?","1) git add 2) git commit 3) git push",""
"4 Coding","a Linux","git fetch","pulls files from remote repo to local repo",""
"4 Coding","a Linux","git merge","pulls files from local repo to working directory",""
"4 Coding","a Linux","-m command","-m ""MESSAGE""  Example: git commit -m ""my message""",""
"4 Coding","a Linux","git diff","show differences between now and most recent commit",""
"4 Coding","a Linux","git checkout FILENAME","undo git add and git commit",""
"4 Coding","a Linux","git branch branchname","create a branch",""
"4 Coding","a Linux","git branch","show all project branches",""
"4 Coding","a Linux","git checkout branchname","switch to a branch",""
"4 Coding","a Linux","git tag","show tags (major landmarks)",""
"4 Coding","a Linux","git tag tagname","create a tag",""
"4 Coding","a Linux","What is the purpose of .gitignore?","It is a text document to tell github which items it can ignore and not upload to the repo",""
"4 Coding","a Linux","determine the current shell in Linux","echo $SHELL",""
"4 Coding","a Linux","What does ""man"" do in Linux","It helps show documentation for functions.  Ex: man ls  gives you documentation for ls",""
"4 Coding","a Linux","pwd in Linux","Present working directory (pwd) prints the absolute path to your current directory",""
"4 Coding","a Linux","ls in Linux","Tells you what is in the current directory.",""
"4 Coding","a Linux","-a flag in Linux","Selects all files.   Ex: ls -a  lists all files, even hidden folders  Ex: git commit -a  commit all files that have been staged",""
"4 Coding","a Linux","-l flag in Linux","Gives a detailed list of the output  Ex: ls -l  detailed contents of a directory",""
"4 Coding","a Linux","[Linux]  ..","denotes the parent of current working directory",""
"4 Coding","a Linux","[Linux]  .","denotes the current working directory",""
"4 Coding","a Linux","[Linux]  ~","denotes the home directory",""
"4 Coding","a Linux","[Linux]  /","denotes the root directory",""
"4 Coding","a Linux","[Linux]  How to go to the parent (previous) directory, home directory, and root directory","Parent directory = cd .. Home directory = cd OR cd ~ Root directory = cd /",""
"4 Coding","a Linux","[Linux]  How to copy a file","cp <file name> <target location>",""
"4 Coding","a Linux","[Linux]  How to move a file","mv <file name> <new location>",""
"4 Coding","a Linux","[Linux]  How to create a a text file","touch <name>.txt",""
"4 Coding","a Linux","[Linux]  How to create a new directory","mkdir <directory name>",""
"4 Coding","a Linux","[Linux]  How to delete a file","rm <file name>",""
"4 Coding","a Linux","[Linux]  How to delete a directory and everything in it","rm -rf <directory name>",""
"4 Coding","a Linux","[Linux]  Identify the following wildcard characters:  ?, *, +, ^, $","? = any single character * = any amount of any characters + = one or more preceding pattern ^ = beginning of the line $ = end of the line",""
"4 Coding","a Linux","[Linux]  How to take a sneak peak of the contents of a file","cat <file name> to print the entire file  head <file name>  for just the first 10 lines  head -15 <file name> for just the first 15 lines  tail <file name> for just the last 10 lines",""
"4 Coding","a Linux","[Linux]  What does -n flag usually mean?","Usually followed by a number argument.  Ex: tail -n +20 temp.R  prints out the last lines of code starting from line 20",""
"4 Coding","a Linux","[Linux]  What does a pipe (|) do?","A pipe sends output from one command as input for another command.  Ex: ls -l | head -5  the (head -5) argument is applied to the list generated by (ls -l)",""
"4 Coding","a Linux","[Linux]  What do >, >>, and < do?",">  directs output from one command to a file  It implicitly erases whatever might have been there before.  >> appends output from one command to a file  It does not erase anything but rather appends  < reads input from a file",""
"4 Coding","a Linux","[Linux]  What do ""less"" and ""more"" do?","Opens a file and you can read it",""
"4 Coding","a Linux","[Linux]  grep","prints lines that match an expression.  Ex: grep ""word"" temp.txt  and it will print lines that have ""word"" in it",""
"4 Coding","a Linux","[Linux]  sed","""sed"" stands for stream editor  It can search, find and replace, insert, or replace phrases in text files.  Ex: sed 'sed/CentOS/RHEL/' linux.qmd  replaces CentOS by RHEL",""
"4 Coding","a Linux","[Linux]  awk","""awk"" is a function that can filter and write reports  the syntax is as follows:  awk options 'selection _criteria {action }' input-file > output-file",""
"4 Coding","a Linux","[Linux]  How to use ""awk"" to print lines","awk '{print}' xxx.txt",""
"4 Coding","a Linux","[Linux]  What are the built-in variables for ""awk""?   NR, NF, FS, RS, OFS, ORS","NR = current line number NF = number of fields in the current line FS =  RS =  OFS =  ORS =",""
"4 Coding","a Linux","[Linux]  wc","line count, or word count   Ex: wc",""
"6 Survival Analysis",NA,"What is a survival function?","The probability of an individual surviving until time t  Continuous:  [$]S(t) = P(T > t) = \int_t^{\infty} f(t) dt[/$]  Discrete:  [$]S(t) = P(T > t) = \sum_{t_j > t} P(t_j)[/$]",""
"6 Survival Analysis",NA,"Hazard function (risk function)","The chance an individual experiences the event in the next instant in time.  Continuous:  [$]h(t) = \lim_{\Delta t  \rightarrow 0} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t}[/$]  Discrete:  [$]h(t_j) = P(T = t_j \mid T \geq t_j) = \frac{P(t_j)}{S(t_{j-1})}[/$]  [$]S(t) = \prod_{t_j \leq t} \frac{S(t_j)}{S(t_{j-1})}[/$]  [$]S(t) = \prod_{t_j \leq t} \left[1 - h(t_j)\right][/$]",""
"6 Survival Analysis",NA,"How does the survival function relate to the CDF?  [$]S(t), F(t)[/$]","[$]S(t) = 1 - F(t)[/$]",""
"6 Survival Analysis",NA,"How do the hazard function, cumulative hazard function, and survival function relate to each other?","[$]h(t) = \frac{f(t)}{S(t)} = -\frac{d \ln{(S(t))}}{dt}[/$]  [$]H(t) = \int_0^t h(u) du = - \ln{(S(t))}[/$]",""
"9 Extras",NA,"How to clone a git repo but give it a new name","git clone [URL] new_name",""
"9 Extras",NA,"How to update your remote tracking branches","git fetch",""
"9 Extras",NA,"Add a new branch","git branch [branch name]",""
"9 Extras",NA,"change git branch","git checkout [new branch name]",""
"9 Extras",NA,"how to merge branches","1) have your HEAD point to the main branch  git checkout [main branch name]  2) merge them  git merge [secondary branch name]",""
"9 Extras",NA,"how to move a local branch to sync with the remove branch","First, make sure that you are on the correct branch:  git checkout [branch name]  Then, you do a git pull  git pull origin master",""
"9 Extras",NA,"how to delete a branch in git","git branch -D [branch name]",""
"Matrix Derivatives",NA,"[$]\frac{\partial}{\partial \mathbf{x}} \mathbf{Ax}[/$]","[$]\mathbf{A}[/$]",""
"Matrix Derivatives",NA,"[$]\frac{\partial}{\partial y} \mathbf{yAx}[/$]","[$]\mathbf{x'A'}[/$]",""
"Optimization",NA,"Lagrange Multipliers","If we want to minimize a function f(x) subject to g(x) = 0, then the lgrangian is:  [$]L(x, \lambda) = f(x) + \lambda \cdot g(x)[/$]",""
"Optimization",NA,"Second Order Taylor Expansion around  [$]f(\mathbf{x} + \Delta \mathbf{x})[/$]  for a vector [$]\mathbf{x}[/$]","[$]f(\mathbf{x} + \Delta \mathbf{x}) = f(\mathbf{x}) + \nabla f(\mathbf{x})' \Delta \mathbf{x} + \frac{1}{2} \delta \mathbf{x}' \nabla^2 f(\mathbf{x}) \Delta \mathbf{x}[/$]",""
"Optimization",NA,"Newton Raphson Step","[$] \Delta \mathbf{x} = - \left[\nabla^2 f(\mathbf{x})\right]^{-1} \nabla f(\mathbf{x})[/$]",""
NA,NA,"[$]\text{Cov}(\mathbf{AX}, \mathbf{BY})[/$]","[latex]$ \mathbf{A} \text{Cov}(\mathbf{X},\mathbf{Y}) \mathbf{B}'$[/latex]",""
NA,NA,"if [$]\mathbf{y} \sim \mathcal{N}_n(\boldsymbol{\mu}, \boldsymbol{\Sigma})[/$], what is the distribution of [$]\mathbf{Cy} + \mathbf{d}[/$]  where [$]\mathbf{C}[/$] is a [$]m \times n[/$] matrix of constants, and [$]\mathbf{d}[/$] is a [$]m \times 1[/$] vector of constants","[$]\mathbf{Cy} + \mathbf{d} \sim \mathcal{N}_m(\mathbf{C}\boldsymbol{\mu} + \mathbf{d}, \mathbf{C}\boldsymbol{\Sigma}\mathbf{C}')[/$]",""
NA,NA,"For random matrix [$]\mathbf{Z}[/$] and constant matrices [$]\mathbf{A,B,C}[/$], what is  [$]\mathbb{E}[\mathbf{AZB + C}][/$]","[$]\mathbf{A} \mathbb{E}[\mathbf{Z}] \mathbf{B} + \mathbf{C}[/$]",""
NA,NA,"Expectation of Discrete Random Variable","[$]\mathbb{E}[X] = \sum_i x_i \cdot p_X(X = x_i)[/$]","1_Intro_Stats"
NA,NA,"Expectation of a function of a discrete random variable   [$]\mathbb{E}[g(X)][/$]","[$]\mathbb{E}[g(X)] = \sum_i g(x_i) \cdot p_X(X = x_i)[/$]","1_Intro_Stats"
NA,NA,"Expectation of a continuous random variable","[$]\int x \cdot f_X(x) dx[/$]","1_Intro_Stats"
NA,NA,"Expectation of a function of a continuous random variable   [$]\mathbb{E}[g(X)][/$]","[$]\mathbb{E}[g(X)] = \int g(x) f_X(x) dx[/$]","1_Intro_Stats"
NA,NA,"Definition of Variance of a random variable","[$]Var(X) = \mathbb{E}\left[\left(X - \mathbb{E}[X]\right)^2\right][/$]","1_Intro_Stats"
NA,NA,"Variance as a function of moments","[$]Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2[/$]","1_Intro_Stats"
NA,NA,"Variance of a linear combination of random variables  [$]Var(aX + b)[/$]","[$]a^2 \cdot Var(X)[/$]","1_Intro_Stats"
NA,NA,"Variance of sum of two random variables  [$]Var(X + Y)[/$]","[$]Var(X) + Var(Y) + 2 \cdot Cov(X, Y)[/$]","1_Intro_Stats"
NA,NA,"Variance of a sum of random variables  [$]Var\left(\sum_{i=1}^n X_i\right)[/$]","[$]Var\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n Var(X_i) \newline + \sum_{i=1}^n \sum_{j \neq i} Cov(X_i, X_j)[/$]","1_Intro_Stats"
NA,NA,"Definition of covariance","[$]Cov(X, Y) = \mathbb{E}\left[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])\right][/$]","1_Intro_Stats"
NA,NA,"Covariance as a function of moments","[$]Cov(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y][/$]","1_Intro_Stats"
NA,NA,"Covariance as an integral of a continuous random variable","[$]Cov(X,Y) = \int \int (x - \mu_x)(y - \mu_y) f_{X,Y}(x,y) dx dy[/$]","1_Intro_Stats"
NA,NA,"Covariance of two discrete random variables  Use two summations","[$]Cov(X,Y) = \sum \sum (x - \mu_x)(y - \mu_y) f_{X,Y}(x,y)[/$]","1_Intro_Stats"
NA,NA,"[$]Cov(X + Y, Z)[/$]","[$]Cov(X, Z) + Cov(Y, Z)[/$]","1_Intro_Stats"
NA,NA,"[$]Cov(aX + b, cY + d)[/$]","[$]ac \cdot Cov(X,Y)[/$]","1_Intro_Stats"
NA,NA,"Correlation of two random variables formula","[$]Cor(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X) \cdot Var(Y)}}[/$]","1_Intro_Stats"
NA,NA,"nth moment of a random variable","[$]\mu_n' = \mathbb{E}[X^n][/$]","1_Intro_Stats"
NA,NA,"nth central moment of a random variable","[$]\mu_n = \mathbb{E}\left[(X - \mu)^n\right][/$]","1_Intro_Stats"
NA,NA,"Probability generating function (PGF)","[$]G(z) = \mathbb{E}\left[z^X\right] = \sum P(X = x)z^x[/$]","1_Intro_Stats"
NA,NA,"Moment generating function (MGF)","[$]M_X(t) = \mathbb{E}\left[e^{tX}\right][/$]","1_Intro_Stats"
NA,NA,"Calculating moments from MGF's","[$]\mathbb{E}\left[X^n\right] = \left. M_X^{(n)}(0) = \frac{d^n M_X}{dt^n} \right|_{t = 0}[/$]","1_Intro_Stats"
NA,NA,"MGF of linear transformation of random variable  [$]M_{\alpha X + \beta}(t)[/$]","[$]M_{\alpha X + \beta}(t) = \mathbb{E}\left[e^{(\alpha X + \beta) t}\right] = e^{\beta t} M_X(\alpha t)[/$]","1_Intro_Stats"
NA,NA,"Markov Inequality","[$]P(X \geq a) \leq \frac{\mathbb{E}[X]}{a}[/$]","1_Intro_Stats"
NA,NA,"Chebyshev's Inequality","[$]P(|X - \mathbb{E}[X]| \geq a) \leq \frac{Var(X)}{a^2}[/$]","1_Intro_Stats"
NA,NA,"If [$]X \sim Pois(\lambda_1)[/$] and [$] Y \sim Pois(\lambda_2)[/$], then what is [$]W = X + Y[/$]","[$] W = X + Y \sim Pois(\lambda_1 + \lambda_2)[/$]","1_Intro_Stats"
NA,NA,"[$]\chi^2_k \equiv Gamma(x,y)[/$]. What are x and y?","[$]\chi^2_k \equiv Gamma\left(\frac{k}{2}, 2\right)[/$]","1_Intro_Stats"
NA,NA,"If [$]X \sim t_{\nu}[/$], then what is the distribution of [$]X^2[/$]?","[$]X^2 \sim F_{1, \nu}[/$]","1_Intro_Stats"
NA,NA,"Marginal PMF formula","[$]f_X(x) = \sum_y f_{X,Y}(x,y)[/$] [$]f_Y(y) = \sum_x f_{X,Y}(x,y)[/$]","1_Intro_Stats"
NA,NA,"Marginal PDF formula","[$]f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) dy[/$] [$]f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) dx[/$]","1_Intro_Stats"
NA,NA,"Joint PDF as a function of joint CDF","[$]f_{X,Y}(x,y) = \frac{\partial^2 F(x,y)}{\partial x \partial y}[/$]","1_Intro_Stats"
NA,NA,"Bivariate CDF formula","[$]F_{X,Y}(x,y) = P(X \leq x, Y \leq y)[/$]","1_Intro_Stats"
NA,NA,"Conditional PDF formula as a function of the joint PDF","[$]f_{Y | X}(y | x) = \frac{f_{X,Y}(x,y)}{f_X(x)}[/$]","1_Intro_Stats"
NA,NA,"Conditional Expectation formula  [$]\mathbb{E}[X | Y = y][/$]","[$]\mathbb{E}[X | Y = y] = \int x f(x|y) dx = \int x \frac{f_{X,Y}(x,y)}{f_Y(y)}dx[/$]","1_Intro_Stats"
NA,NA,"Simplify the conditional expectation  [$]\mathbb{E}[g(Y) | x][/$]","[$]\mathbb{E}[g(Y) | x] = \int g(y) \frac{f_{X,Y}(x,y)}{f_X(x)} dy[/$]","1_Intro_Stats"
NA,NA,"Law of Total Expectation","[$]\mathbb{E}[\mathbb{E}[X | Y]] = \mathbb{E}[X][/$]","1_Intro_Stats"
NA,NA,"Law of Total Variance","[$]Var(Y) = \mathbb{E}[Var(Y|X)] + Var(\mathbb{E}[Y|X])[/$]","1_Intro_Stats"
NA,NA,"CDF of first order statistic (lowest value)","[$] F_{X_{(1)}}(x) = 1 - (1 - F(x))^n[/$]","1_Intro_Stats"
NA,NA,"PDF of first order statistic where CDF is  [$]F_{X_{(1)}}(x) = 1 - (1 - F(x))^n[/$]","[$]f_{X_{(1)}}(x) = n (1 - F(x))^{n-1} f(x)[/$]","1_Intro_Stats"
NA,NA,"CDF of highest order statistic","[$]F_{X_{(n)}}(x) = F(x)^n[/$]","1_Intro_Stats"
NA,NA,"PDF of highest order statistic given the CDF:  [$]F_{X_{(n)}} = F(x)^n[/$]","[$]f_{X_{(n)}}(x) = n F(x)^{n-1} f(x)[/$]","1_Intro_Stats"
NA,NA,"Exponential family form","[$]f_X(x : \boldsymbol{\theta}) = h(x) \exp{\left( \boldsymbol{\eta}(\boldsymbol{\theta})^{\top} \mathbf{T}(x) - A(\boldsymbol{\theta})\right)}[/$]","1_Intro_Stats"
NA,NA,"Find the canonical link function in the exponential family:  [$]f_X(x : \boldsymbol{\theta}) = h(x) \exp{\left( \boldsymbol{\eta}(\boldsymbol{\theta})^{\top} \mathbf{T}(x) - A(\boldsymbol{\theta})\right)}[/$]","[$]\boldsymbol{\eta}(\boldsymbol{\theta})[/$]","1_Intro_Stats"
NA,NA,"Find the sufficient statistic in the exponential family  [$]f_X(x : \boldsymbol{\theta}) = h(x) \exp{\left( \boldsymbol{\eta}(\boldsymbol{\theta})^{\top} \mathbf{T}(x) - A(\boldsymbol{\theta})\right)}[/$]","[$]\mathbf{T}(x)[/$]","1_Intro_Stats"
NA,NA,"Central Limit Theorem","If [$]X_1, \ldots, X_n[/$] are iid random variables with mean [$]\mu[/$] and variance [$]\sigma^2[/$], then the distribution of the sample mean will be asymototically normal:  [$]\overline{X}\sim \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right)[/$]","1_Intro_Stats"
NA,NA,"Sufficient Statistic definition","A statistic [$]T(\mathbf{X})[/$] is a sufficient statistic for [$]\theta[/$] if the conditional distribution of [$]\mathbf{X}[/$] given the value of [$]T(\mathbf{X})[/$] does not depend on [$]\theta[/$]","1_Intro_Stats"
NA,NA,"Ancillary Statistic","A statistic [$]S(\mathbf{X})[/$] whose sampling distribution does not depend on the parameter [$]\theta[/$]","1_Intro_Stats"
NA,NA,"Complete Statistic","???","1_Intro_Stats"
NA,NA,"Method of Moments Estimator","[$]\hat{\mu}_{k} = \frac{X_1^k + \cdots + X_n^k}{n}[/$]","1_Intro_Stats"
NA,NA,"Likelihood function formula with density [$]f(x | \pi)[/$]","[$] l(\pi) = \prod f(x_i | \pi)[/$]","1_Intro_Stats"
NA,NA,"Log-Likelihood as a function of the densities [$] f(x | \pi)[/$]","[$]L(\pi) = \sum_i \log{f(x_i | \pi)}[/$]","1_Intro_Stats"
NA,NA,"Score function as a function of density [$]f(x | \pi)[/$]","[$]S(\pi) = \sum_i \frac{\partial}{\partial \pi} \log{f(x_i | \pi)}[/$]","1_Intro_Stats"
NA,NA,"Fisher information of a random variable as a function of log-likelihood","[$]\mathcal{I}(\pi) = -\frac{\partial^2}{\partial \pi^2} L(\pi)[/$]","1_Intro_Stats"
NA,NA,"Exepctation of score function","[$]\mathbb{E}[S(\pi)] = 0[/$]","1_Intro_Stats"
NA,NA,"Variance of score function","[$]Var(S(\theta)) = \mathcal{I}(\theta)[/$]","1_Intro_Stats"
NA,NA,"Equations for Expected Fisher Information","[$]\mathcal{I}(\theta) = Var(S(\theta)) = \mathbb{E}[S(\theta)^2] = -\mathbb{E}[\mathcal{J}(\theta)][/$]","1_Intro_Stats"
NA,NA,"Asymptotic Distribution of MLE","[$]\sqrt{n}(\hat{\theta}_{MLE} - \theta) \sim \mathcal{N}(0, \mathcal{I}(\theta)^{-1})[/$]","1_Intro_Stats"
NA,NA,"Wald Test for Parameter Estimation","Using the asymtotically normal distribution of the MLE, we approximate using the normal distribution:  [$]z = \frac{(\hat{\theta} - \theta_0)}{\sqrt{\mathcal{I}(\hat{\theta})}} \sim \mathcal{N}(0,1)[/$]","1_Intro_Stats"
NA,NA,"Score Test for Parameter Estimation","Using the distribution of the score function:  [$]","1_Intro_Stats"
NA,NA,"Multivariate Wald Test","[$]n(\hat{\theta} - \theta_0)^{\top} \mathcal{J}(\theta_0) (\hat{\theta} - \theta_0) \rightarrow \chi_d^2[/$]  where [$]d[/$] is the number of parameters in [$]\theta[/$]","1_Intro_Stats"
NA,NA,"Multivariate Score Test","[$]\frac{1}{n} \textbf{u}(\theta_0)^{\top} \mathcal{J}(\theta_0)^{-1} \mathbf{u}(\theta_0) \rightarrow \chi^2_d[/$]  where [$]d[/$] is the number of parameters in [$]\theta[/$]","1_Intro_Stats"
NA,NA,"Likelihood Ratio Test","[$] 2 \log{\left(\frac{L(\hat{\theta})}{L(\theta_0)}\right)} \rightarrow \chi^2_d[/$]","1_Intro_Stats"
